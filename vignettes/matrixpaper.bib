@article{anderlucci2015,
author = "Anderlucci, Laura and Viroli, Cinzia",
doi = "10.1214/15-AOAS816",
fjournal = "The Annals of Applied Statistics",
journal = "Ann. Appl. Stat.",
month = "06",
number = "2",
pages = "777--800",
publisher = "The Institute of Mathematical Statistics",
title = "Covariance pattern mixture models for the analysis of multivariate heterogeneous longitudinal data",
url = "https://doi.org/10.1214/15-AOAS816",
volume = "9",
year = "2015"
}

@article{viroli2011,
author = "Viroli, Cinzia",
doi = "10.1214/11-BA622",
fjournal = "Bayesian Analysis",
journal = "Bayesian Anal.",
month = "12",
number = "4",
pages = "573--602",
publisher = "International Society for Bayesian Analysis",
title = "Model based clustering for three-way data structures",
url = "https://doi.org/10.1214/11-BA622",
volume = "6",
year = "2011"
}


@Article{Viroliclass2011,
author="Viroli, Cinzia",
title="Finite mixtures of matrix normal distributions for classifying three-way data",
journal="Statistics and Computing",
year="2011",
month="Oct",
day="01",
volume="21",
number="4",
pages="511--522",
abstract="Matrix-variate distributions represent a natural way for modeling random matrices. Realizations from random matrices are generated by the simultaneous observation of variables in different situations or locations, and are commonly arranged in three-way data structures. Among the matrix-variate distributions, the matrix normal density plays the same pivotal role as the multivariate normal distribution in the family of multivariate distributions. In this work we define and explore finite mixtures of matrix normals. An EM algorithm for the model estimation is developed and some useful properties are demonstrated. We finally show that the proposed mixture model can be a powerful tool for classifying three-way data both in supervised and unsupervised problems. A simulation study and some real examples are presented.",
issn="1573-1375",
doi="10.1007/s11222-010-9188-x",
url="https://doi.org/10.1007/s11222-010-9188-x"
}



@article{dogru2016,
author = {Doğru, Fatma and Bulut, Y. Murat and Arslan, Olcay},
year = {2016},
month = {06},
pages = {335-341},
title = {Finite Mixtures of Matrix Variate t Distributions},
volume = {29},
journal = {Gazi University Journal of Science}
}

@article{gallaugher2017,
author = {Gallaugher, Michael P.B. and McNicholas, Paul D.},
title = {A matrix variate skew-t distribution},
year = {2017},
journal = {Stat},
volume = {6},
number = {1},
pages = {160-170},
keywords = {matrix variate distribution, skew-t distribution},
doi = {10.1002/sta4.143},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.143},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.143},
abstract = {Although there is ample work in the literature dealing with skewness in the multivariate setting, there is a relative paucity of work in the matrix variate paradigm. Such work is, for example, useful for modelling three-way data. A matrix variate skew-t distribution is derived based on a mean-variance matrix normal mixture. An expectation-conditional maximization algorithm is developed for parameter estimation. Simulated data are used for illustration. Copyright © 2017 John Wiley \& Sons, Ltd.}
}

@article{liurubin1994,
 ISSN = {00063444},
 DOI = {10.2307/2337067},
 abstract = {A generalisation of the ECM algorithm (Meng & Rubin, 1993), which is itself an extension of the EM algorithm (Dempster, Laird & Rubin, 1977), can be obtained by replacing some CM-steps of ECM, which maximise the constrained expected complete-data loglikelihood function, with steps that maximise the correspondingly constrained actual likelihood function. This algorithm, which we call ECME algorithm, for Expectation/Conditional Maximisation Either, shares with both EM and ECM their stable monotone convergence and basic simplicity of implementation relative to competing faster converging methods. Moreover, ECME can have a substantially faster convergence rate than either EM or ECM, measured using either the number of iterations or actual computer time. There are two reasons for this improvement. First, in some of ECME's maximisation steps, the actual likelihood is being conditionally maximised, rather than a current approximation to it, as with EM and ECM. Secondly, ECME allows faster converging numerical methods to be used on only those constrained maximisations where they are most efficacious. Illustrative ECME algorithms are presented with both closed-form and iterative CM-steps, which demonstrate the faster rate of convergence and the associated easier assessment of convergence. Also, theoretical expressions are presented and illustrated regarding the rate of convergence of ECME. Finally, relationships with Markov chain Monte Carlo methods are discussed.},
 author = {Chuanhai Liu and Donald B. Rubin},
 journal = {Biometrika},
 number = {4},
 pages = {633--648},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The {ECME} Algorithm: A Simple Extension of {EM} and {ECM} with Faster Monotone Convergence},
 volume = {81},
 year = {1994}
}

@article{ROY2005462,
title = "On discrimination and classification with multivariate repeated measures data",
journal = "Journal of Statistical Planning and Inference",
volume = "134",
number = "2",
pages = "462 - 485",
year = "2005",
issn = "0378-3758",
doi = "10.1016/j.jspi.2004.04.012",
url = "http://www.sciencedirect.com/science/article/pii/S0378375804002186",
author = "Anuradha Roy and Ravindra Khattree",
keywords = "Classification rule, Compound symmetry, Covariance structures, Maximum likelihood estimates, Repeated measures data, Structure on mean",
abstract = "We study the problem of classification with multiple q-variate observations with and without time effect on each individual. We develop new classification rules for populations with certain structured and unstructured mean vectors and under certain covariance structures. The new classification rules are effective when the number of observations is not large enough to estimate the variance–covariance matrix. Computational schemes for maximum likelihood estimates of required population parameters are given. We apply our findings to two real data sets as well as to a simulated data set."
}

@article{dutilleul1999mle,
  title={The {MLE} algorithm for the matrix normal distribution},
  author={Dutilleul, Pierre},
  journal={Journal of statistical computation and simulation},
  volume={64},
  number={2},
  pages={105--123},
  year={1999},
  publisher={Taylor \& Francis}
}

@article{Fisher,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2331838},
 author = {R. A. Fisher},
 journal = {Biometrika},
 number = {4},
 pages = {507--521},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Frequency Distribution of the Values of the Correlation Coefficient in Samples from an Indefinitely Large Population},
 volume = {10},
 year = {1915}
}

@article{rao1948tests,
  title={Tests of significance in multivariate analysis},
  author={Rao, C Radhakrishna},
  journal={Biometrika},
  volume={35},
  number={1/2},
  pages={58--79},
  year={1948},
  publisher={JSTOR}
}


@article{AndersonBahadur,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2237521},
 abstract = {Linear procedures for classifying an observation as coming from one of two multivariate normal distributions are studied in the case that the two distributions differ both in mean vectors and covariance matrices. We find the class of admissible linear procedures, which is the minimal complete class of linear procedures. It is shown how to construct the linear procedure which minimizes one probability of misclassification given the other and how to obtain the minimax linear procedure; Bayes linear procedures are also discussed.},
 author = {T. W. Anderson and R. R. Bahadur},
 journal = {The Annals of Mathematical Statistics},
 number = {2},
 pages = {420--431},
 publisher = {Institute of Mathematical Statistics},
 title = {Classification into two Multivariate Normal Distributions with Different Covariance Matrices},
 volume = {33},
 year = {1962}
}

@book{gupta1999matrix,
  title={Matrix variate distributions},
  author={Gupta, Arjun K and Nagar, Daya K},
  volume={104},
  year={1999},
  publisher={CRC Press}
}


@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the {EM} algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society. Series B (methodological)},
  pages={1--38},
  year={1977},
  publisher={JSTOR}
}

@article{lange1989robust,
  title={Robust statistical modeling using the t distribution},
  author={Lange, Kenneth L and Little, Roderick JA and Taylor, Jeremy MG},
  journal={Journal of the American Statistical Association},
  volume={84},
  number={408},
  pages={881--896},
  year={1989},
  publisher={Taylor \& Francis Group}
}

@article{meng1993,
author = {Meng, Xiao-Li and Rubin, Donald B.},
title = {Maximum likelihood estimation via the {ECM} algorithm: A general framework},
journal = {Biometrika},
volume = {80},
number = {2},
pages = {267-278},
year = {1993},
doi = {10.1093/biomet/80.2.267},
URL = {http://dx.doi.org/10.1093/biomet/80.2.267},
eprint = {/oup/backfile/content_public/journal/biomet/80/2/10.1093/biomet/80.2.267/2/80-2-267.pdf}
}








@inproceedings{bf88260936d14048a3d6bc78fc588842,
title = "Non-iterative two-dimensional linear discriminant analysis",
abstract = "Linear discriminant analysis (LDA) is a well-known scheme for feature extraction and dimensionality reduction of labeled data in a vector space. Recently, LDA has been extended to two-dimensional LDA (2DLDA), which is an iterative algorithm for data in matrix representation. In this paper, we propose non-iterative algorithms for 2DLDA. Experimental results show that the non-iterative algorithms achieve competitive recognition rates with the iterative 2DLDA, while they are computationally more efficient than the iterative 2DLDA.",
author = "Kohei Inoue and Kiichi Urahama",
year = "2006",
doi = "10.1109/ICPR.2006.860",
language = "English",
isbn = "0769525210",
volume = "2",
pages = "540--543",
booktitle = "Proceedings - 18th International Conference on Pattern Recognition, ICPR 2006",

}






@article{HUANG20103195,

title = "Fusion {(2D)2PCALDA}: A new method for face recognition",

journal = "Applied Mathematics and Computation",

volume = "216",

number = "11",

pages = "3195 - 3199",

year = "2010",

issn = "0096-3003",

doi = "10.1016/j.amc.2010.04.042",

url = "http://www.sciencedirect.com/science/article/pii/S0096300310004686",

author = "Guohong Huang",

keywords = "Fusion face image, F(2D)PCA, LDA, Face recognition, Feature extraction",

abstract = "This paper proposes an efficient face representation and recognition method, which combines the both information between rows and those between columns from two-directional 2DPCA on fusion face image and the optimal discriminative information from column-directional 2DLDA. Experiment results on ORL and Yale face database demonstrate the effectiveness of the proposed method."

}








@article{inoue2011,


  title={Non-iterative Symmetric Two-Dimensional Linear Discriminant Analysis},


  author={Kohei Inoue and Kenji Hara and Kiichi Urahama},


  journal={IEICE Transactions on Information and Systems},


  volume={E94.D},


  number={4},


  pages={926-929},


  year={2011},


  doi={10.1587/transinf.E94.D.926}


}






@article{yang2004uncorrelated,
author = {Yang, JIAN and Yang, JING-YU and Frangi, ALEJANDRO F. and Zhang, DAVID},
title = {UNCORRELATED PROJECTION DISCRIMINANT ANALYSIS AND ITS APPLICATION TO FACE IMAGE FEATURE EXTRACTION},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
volume = {17},
number = {08},
pages = {1325-1347},
year = {2003},
doi = {10.1142/S0218001403002903},

URL = { 
        https://doi.org/10.1142/S0218001403002903
    
},
eprint = { 
        https://doi.org/10.1142/S0218001403002903
    
}
,
    abstract = { In this paper, a novel image projection analysis method (UIPDA) is first developed for image feature extraction. In contrast to Liu's projection discriminant method, UIPDA has the desirable property that the projected feature vectors are mutually uncorrelated. Also, a new LDA technique called EULDA is presented for further feature extraction. The proposed methods are tested on the ORL and the NUST603 face databases. The experimental results demonstrate that: (i) UIPDA is superior to Liu's projection discriminant method and more efficient than Eigenfaces and Fisherfaces; (ii) EULDA outperforms the existing PCA plus LDA strategy; (iii) UIPDA plus EULDA is a very effective two-stage strategy for image feature extraction. }
}







@article{Li2005SLD,
 author = {Li, Ming and Yuan, Baozong},
 title = {2D-{LDA}: A Statistical Linear Discriminant Analysis for Image Matrix},
 journal = {Pattern Recogn. Lett.},
 issue_date = {April, 2005},
 volume = {26},
 number = {5},
 month = apr,
 year = {2005},
 issn = {0167-8655},
 pages = {527--532},
 numpages = {6},
 url = {http://dx.doi.org/10.1016/j.patrec.2004.09.007},
 doi = {10.1016/j.patrec.2004.09.007},
 acmid = {1750509},
 publisher = {Elsevier Science Inc.},
 address = {New York, NY, USA},
 keywords = {Face recognition, Feature extraction, Image representation, Linear discriminant analysis, Subspace techniques},
} 





@article{Lu2009UMD,
 author = {Lu, Haiping and Plataniotis, Konstantinos N. and Venetsanopoulos, Anastasios N.},
 title = {Uncorrelated Multilinear Discriminant Analysis with Regularization and Aggregation for Tensor Object Recognition},
 journal = {Trans. Neur. Netw.},
 issue_date = {January 2009},
 volume = {20},
 number = {1},
 month = jan,
 year = {2009},
 issn = {1045-9227},
 pages = {103--123},
 numpages = {21},
 url = {http://dx.doi.org/10.1109/TNN.2008.2004625},
 doi = {10.1109/TNN.2008.2004625},
 acmid = {1657484},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Dimensionality reduction, dimensionality reduction, face recognition, feature extraction, fusion, gait recognition, multilinear discriminant analysis, regularization, tensor objects},
}





@inproceedings{luo2009symmetric,
  title={Symmetric two dimensional linear discriminant analysis {(2DLDA)}},
  author={Luo, Dijun and Ding, Chris and Huang, Heng},
  booktitle={Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
  pages={2820--2827},
  year={2009},
  organization={IEEE}
}




@inproceedings{mahanta2014ranking,
  title={Ranking {2DLDA} features based on fisher discriminance},
  author={Mahanta, Mohammad Shahin and Plataniotis, Konstantinos N},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
  pages={8307--8311},
  year={2014},
  organization={IEEE}
}




@article{noushath20062d,
  title={{(2D) 2LDA}: An efficient approach for face recognition},
  author={Noushath, S and Kumar, G Hemantha and Shivakumara, P},
  journal={Pattern recognition},
  volume={39},
  number={7},
  pages={1396--1400},
  year={2006},
  publisher={Elsevier}
}




@article{LI201592,


title = "Robust L1-norm two-dimensional linear discriminant analysis",


journal = "Neural Networks",


volume = "65",


pages = "92 - 104",


year = "2015",


issn = "0893-6080",


doi = "https://doi.org/10.1016/j.neunet.2015.01.003",


url = "http://www.sciencedirect.com/science/article/pii/S0893608015000258",


author = "Chun-Na Li and Yuan-Hai Shao and Nai-Yang Deng",


keywords = "Linear discriminant analysis, Two-dimensional linear discriminant analysis, L1-norm two-dimensional linear discriminant analysis, Dimensionality reduction, Iterative technique",


abstract = "In this paper, we propose an L1-norm two-dimensional linear discriminant analysis (L1-2DLDA) with robust performance. Different from the conventional two-dimensional linear discriminant analysis with L2-norm (L2-2DLDA), where the optimization problem is transferred to a generalized eigenvalue problem, the optimization problem in our L1-2DLDA is solved by a simple justifiable iterative technique, and its convergence is guaranteed. Compared with L2-2DLDA, our L1-2DLDA is more robust to outliers and noises since the L1-norm is used. This is supported by our preliminary experiments on toy example and face datasets, which show the improvement of our L1-2DLDA over L2-2DLDA."


}



@INPROCEEDINGS{safayani2010,
author={M. {Safayani} and M. T. M. {Shalmani}},
booktitle={2010 20th International Conference on Pattern Recognition},
title={Heteroscedastic Multilinear Discriminant Analysis for Face Recognition},
year={2010},
volume={},
number={},
pages={4287-4290},
keywords={face recognition;learning (artificial intelligence);heteroscedastic multilinear discriminant analysis;face recognition;subspace learning;tensor-based approach;high dimensional space;face database;Databases;Face;Covariance matrix;Training;Accuracy;Estimation;Optimization;Feature Extraction;Heteroscedastic Problem;Multilinear Discriminant Analysis;Face Recognition},
doi={10.1109/ICPR.2010.1042},
ISSN={1051-4651},
month={Aug},}





@article{yan2007multilinear,
  title={Multilinear Discriminant Analysis for Face Recognition},
  author={Yan, Shuicheng and Xu, Dong and Yang, Qiang and Zhang, Lei and Tang, Xiaoou and Zhang, Hong-Jiang},
  journal={IEEE Transactions on Image Processing},
  volume={16},
  number={1},
  year={2007}
}




@incollection{NIPS2004_2547,


title = {Two-Dimensional Linear Discriminant Analysis},


author = {Ye, Jieping and Ravi Janardan and Li, Qi},


booktitle = {Advances in Neural Information Processing Systems 17},


editor = {L. K. Saul and Y. Weiss and L. Bottou},


pages = {1569--1576},


year = {2005},


publisher = {MIT Press},


url = {http://papers.nips.cc/paper/2547-two-dimensional-linear-discriminant-analysis.pdf}


}






@inproceedings{zhang2011improvement,
  title={An improvement to matrix-based {LDA}},
  author={Zhang, Chongyang and Yang, Jingyu},
  booktitle={International Conference on Artificial Intelligence and Computational Intelligence},
  pages={562--568},
  year={2011},
  organization={Springer}
}




@article{zhao2012separable,
  title={Separable linear discriminant analysis},
  author={Zhao, Jianhua and Philip, LH and Shi, Lei and Li, Shulan},
  journal={Computational Statistics \& Data Analysis},
  volume={56},
  number={12},
  pages={4290--4300},
  year={2012},
  publisher={Elsevier}
}




@article{zheng20081d,
  title={{1D-LDA} vs. {2D-LDA}: When is vector-based linear discriminant analysis better than matrix-based?},
  author={Zheng, Wei-Shi and Lai, Jian-Huang and Li, Stan Z},
  journal={Pattern Recognition},
  volume={41},
  number={7},
  pages={2156--2172},
  year={2008},
  publisher={Elsevier}
}



@article{Fisher1936,
author = {Fisher, R. A.},
title = {The use of multiple measurements in taxonomic problems},
journal = {Annals of Eugenics},
volume = {7},
year = {1936},
number = {2},
pages = {179-188},
doi = {10.1111/j.1469-1809.1936.tb02137.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.}
}


     @misc{Dua:2019 ,
    author = "Dua, Dheeru and Graff, Casey",
    year = "2017",
    title = "{UCI} Machine Learning Repository",
    url = "http://archive.ics.uci.edu/ml",
    institution = "University of California, Irvine, School of Information and Computer Sciences" }


@article{fraley2002model,
  title={Model-based clustering, discriminant analysis, and density estimation},
  author={Fraley, Chris and Raftery, Adrian E},
  journal={Journal of the American statistical Association},
  volume={97},
  number={458},
  pages={611--631},
  doi = {10.1198/016214502760047131},
  url = {https://www.tandfonline.com/doi/abs/10.1198/016214502760047131},
  year={2002},
  publisher={Taylor \& Francis}
}


@article{dickey1967,
author = "Dickey, James M.",
doi = "10.1214/aoms/1177698967",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "04",
number = "2",
pages = "511--518",
publisher = "The Institute of Mathematical Statistics",
title = "Matricvariate Generalizations of the Multivariate $t$ Distribution and the Inverted Multivariate $t$ Distribution",
url = "https://www.jstor.org/stable/2239163",
volume = "38",
year = "1967"
}

@InBook{rubin1983,
  author = 	 {Rubin, D.B.},
  ALTeditor = 	 {S. Kotz and N. L. Johnson and C. B. Reed},
  title = 	 {Encyclopedia of Statistical Sciences},
  chapter = 	 {Iteratively Reweighted Least Squares},
  publisher = 	 {John Wiley},
  year = 	 {1983},
  edition = 	 {4},
  pages = 	 {272-5}
}


@article{WOO2014412,
title = "Cluster-extent based thresholding in fMRI analyses: Pitfalls and recommendations",
journal = "NeuroImage",
volume = "91",
pages = "412 - 419",
year = "2014",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2013.12.058",
url = "http://www.sciencedirect.com/science/article/pii/S1053811914000020",
author = "Choong-Wan Woo and Anjali Krishnan and Tor D. Wager",
keywords = "Cluster-extent thresholding, Multiple comparisons, fMRI, Primary threshold, Family-wise error rate, False discovery rate, SPM, FSL, Gaussian random fields",
abstract = "Cluster-extent based thresholding is currently the most popular method for multiple comparisons correction of statistical maps in neuroimaging studies, due to its high sensitivity to weak and diffuse signals. However, cluster-extent based thresholding provides low spatial specificity; researchers can only infer that there is signal somewhere within a significant cluster and cannot make inferences about the statistical significance of specific locations within the cluster. This poses a particular problem when one uses a liberal cluster-defining primary threshold (i.e., higher p-values), which often produces large clusters spanning multiple anatomical regions. In such cases, it is impossible to reliably infer which anatomical regions show true effects. From a survey of 814 functional magnetic resonance imaging (fMRI) studies published in 2010 and 2011, we show that the use of liberal primary thresholds (e.g., p<.01) is endemic, and that the largest determinant of the primary threshold level is the default option in the software used. We illustrate the problems with liberal primary thresholds using an fMRI dataset from our laboratory (N=33), and present simulations demonstrating the detrimental effects of liberal primary thresholds on false positives, localization, and interpretation of fMRI findings. To avoid these pitfalls, we recommend several analysis and reporting procedures, including 1) setting primary p<.001 as a default lower limit; 2) using more stringent primary thresholds or voxel-wise correction methods for highly powered studies; and 3) adopting reporting practices that make the level of spatial precision transparent to readers. We also suggest alternative and supplementary analysis methods."
}



@article{doi:10.1002/mrm.10191,
author = {Maitra, Ranjan and Roys, Steven R. and Gullapalli, Rao P.},
title = {Test-retest reliability estimation of functional MRI data},
journal = {Magnetic Resonance in Medicine},
volume = {48},
number = {1},
pages = {62-70},
keywords = {fMRI, quantification, Markov random field, iterated conditional modes, ROC analysis, motor task},
doi = {10.1002/mrm.10191},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.10191},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.10191},
abstract = {Abstract Functional magnetic resonance imaging (fMRI) data are commonly used to construct activation maps for the human brain. It is important to quantify the reliability of such maps. We have developed statistical models to provide precise estimates for reliability from several runs of the same paradigm over time. Specifically, our method extends the premise of maximum likelihood (ML) developed by Genovese et al. (Magn Reson Med 1997;38:497–507) by incorporating spatial context into the estimation process. Experiments indicate that our methodology provides more conservative estimates of true positives compared to those obtained by Genovese et al. The reliability estimates can be used to obtain voxel-specific reliability measures for activated as well as inactivated regions in future experiments. We derive statistical methodology to determine optimal thresholds for region- and context-specific activations. Empirical guidelines are also provided on the number of repeat scans to acquire in order to arrive at accurate reliability estimates. We report the results from experiments involving a motor paradigm performed on a single subject several times over a period of 2 months. Magn Reson Med 48:62–70, 2002. © 2002 Wiley-Liss, Inc.},
year = {2002}
}


@article{MAITRA2010124,
title = "A re-defined and generalized percent-overlap-of-activation measure for studies of fMRI reproducibility and its use in identifying outlier activation maps",
journal = "NeuroImage",
volume = "50",
number = "1",
pages = "124 - 135",
year = "2010",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2009.11.070",
url = "http://www.sciencedirect.com/science/article/pii/S1053811909012567",
author = "Ranjan Maitra",
keywords = "Eigenvalues, Reliability, Intra-class correlation coefficient, Outlier detection, Percent overlap, Principal components, Finger-thumb opposition experiment, Summarized multiple Jaccard similarity coefficient, Dice coefficient, Perron-Frobenius theorem",
abstract = "Functional Magnetic Resonance Imaging (fMRI) is a popular noninvasive modality to investigate activation in the human brain. The end result of most fMRI experiments is an activation map corresponding to the given paradigm. These maps can vary greatly from one study to the next, so quantifying the reliability of identified activation over several fMRI studies is important. The percent overlap of activation (Rombouts et al., 1998, Machielsen et al., 2000) is a global reliability measure between activation maps drawn from any two fMRI studies. A slightly modified but more intuitive measure is provided by the Jaccard (1901) coefficient of similarity, whose use we study in this paper. A generalization of these measures is also proposed to comprehensively summarize the reliability of multiple fMRI studies. Finally, a testing mechanism to flag potentially anomalous studies is developed. The methodology is illustrated on studies involving left- and right-hand motor task paradigms performed by a right-hand dominant male subject several times over a period of two months, with excellent results."
}


  @Manual{rproject,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019},
    url = {https://www.R-project.org/},
  }


@ARTICLE{fastfmri,
author={I. {Almodóvar-Rivera} and R. {Maitra}},
journal={IEEE Transactions on Medical Imaging},
title={FAST Adaptive Smoothing and Thresholding for Improved Activation Detection in Low-Signal fMRI},
year={2019},
volume={},
number={},
pages={1-1},
keywords={Smoothing methods;Thresholding (Imaging);Functional magnetic resonance imaging;Correlation;Limiting;Random variables;Task analysis;ALL-FAST;AM-FAST;AR-FAST;Adaptive Segmentation;AFNI;BIC;CNR;Cluster Thresholding;SPM;TFCE},
doi={10.1109/TMI.2019.2915052},
ISSN={0278-0062},
month={},}



@article{doi:10.1080/10618600.2018.1476249,
author = {Aaron J. Molstad and Adam J. Rothman},
title = {A Penalized Likelihood Method for Classification With Matrix-Valued Predictors},
journal = {Journal of Computational and Graphical Statistics},
volume = {28},
number = {1},
pages = {11-22},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2018.1476249},

URL = { 
        https://doi.org/10.1080/10618600.2018.1476249
    
},
eprint = { 
        https://doi.org/10.1080/10618600.2018.1476249
    
}

}



@article{Kim2007TensorCC,
  title={Tensor Canonical Correlation Analysis for Action Classification},
  author={Tae-Kyun Kim and Shu-Fai Wong and Roberto Cipolla},
  journal={2007 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2007},
  pages={1-8}
}

@article{teigen,
author = {L. Andrews, Jeffrey and R. Wickins, Jaymeson and Boers, Nicholas and McNicholas, Paul},
year = {2018},
month = {02},
pages = {},
title = {teigen : An R Package for Model-Based Clustering and Classification via the Multivariate t Distribution},
volume = {83},
journal = {Journal of Statistical Software},
doi = {10.18637/jss.v083.i07}
}